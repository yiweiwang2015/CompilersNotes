\ifx\PREAMBLE\undefined
\input{preamble}
\begin{document}
\fi
\chapter{Optimization}
\section{Intermediate code}
Intermediate code uses a language between the source language and the target language. It provides an intermediate level of abstraction: more details than the source and fewer details than the target. High-level source languages like \texttt{Cool} and \texttt{C} reveals less low-level conceptions such as registers, making it difficult to find room for optimization, while low-level languages like assembly language are often limited to a specific type of machine architecture. 

We will introduce the conception with an intermediate language that can be called a ``high level assembly language''. It uses register names, but has an unlimited number of registers. It uses control structures like assembly language. Opcodes are used, but some of them are higher level (e.g. \texttt{\color{red}push}, which will be translated into a few assembly instructions on a machine of a particular architecture). Each instruction is either binary ($\mathtt{\color{red}x\coloneqq y\:op\:z}$) or unary($\mathtt{\color{red}x\coloneqq op\:y}$). Arguments on the right are always registers or constants. This is actually a wide-used form of intermediate code called \textbf{three-address code}. In this language, every intermediate value will have its own name. 

Generating intermediate code is similar to generating assembly code, except that unlimited number of registers can be used, which renders easier code generation. If we use $\color{red}\mathtt{igen(e,t)}$ to denote the function that generates code for expression \texttt{e} and stores the result in register \texttt{t} , we will have 
\begin{align*}
\mathtt{igen}&\mathtt{(e_1 + e_2,t)=}\\
&\mathtt{igen(e_1,t_1)}\\
&\mathtt{igen(e_2,t_2)}\\
&\mathtt{t\coloneqq t_1 + t_2}
\end{align*}
\section{Optimization overview}
Optimizations can be performed at different times:
\begin{enumerate}
\item On AST
\begin{description}
\item[Pro]Machine independent
\item[Con]Too high level
\end{description}
\item On assembly language
\begin{description}
\item[Pro]Exposes the most optimization opportunities
\item[Con]Machine dependent. Has to be reimplemented when re-targeting to different architectures.
\end{description}
\item {\color{red}On intermediate language}
\begin{description}
\item[Pro]Machine independent. Exposes optimization opportunities.
\end{description}
\end{enumerate}
We will discuss optimizations performed on intermediate languages. The intermediate language we use can be described with the following CFG:
\begin{align*}
\mathtt{P\rightarrow\:}&\mathtt{SP\:|\:S}\\
\mathtt{S\rightarrow\:}&\mathtt{id\coloneqq id\:op\:id}\\
|\:&\mathtt{id\coloneqq op\:id}\\
|\:&\mathtt{push\:id}\\
|\:&\mathtt{id\coloneqq pop}\\
|\:&\mathtt{if\:id\:relop\:id\:goto\:L}\\
|\:&\mathtt{L:}\\
|\:&\mathtt{jump\:L}
\end{align*}
\texttt{Id}'s are registers, and can be substituted by constants when they serve as arguments. Typical operations are {\color{red}+,-,*,/}. 

A \textbf{basic block} is a maximal sequence of instructions with no labels (except for the first instruction) and no jumps(except for the last instruction). The execution can only jump into a basic block at the first instruction and jump out of it at the last instruction. There is no other way to jump into it, and all instructions inside the block are guaranteed to be executed sequentially before the execution jumps out. This property enables us to conduct a series of optimizations.

A \textbf{control flow graph} is a directed graph with basic blocks as nodes. An edge from block A to block B exists if the execution can pass from the last instruction in A to the first instruction in B. The body of a method can be represented as a control flow graph. There is one initial node, and all ``return'' nodes are terminals. 

The purpose of optimization is to \textbf{improve a program's resource utilization}. Most of the time we try to make the program run faster, i.e. reduce the execution time. Other resources that optimization could be concerned about are code size, memory footprint, network messages sent, disk accesses, power, etc. The bottom line is that optimization should not alter what the program computes.

For languages like \texttt{C} and \texttt{Cool} there are 3 granularities of optimizations:
\begin{description}
\item[Local optimization]Applies to an isolated basic block.
\item[Global optimization]Applies to an isolated control flow graph (method body).
\item[Inter-procedural optimization]Applies across method boundaries.
\end{description}
Local optimizations are performed by most mainstream compilers. Many of them also perform global optimizations. Few compilers touch inter-procedural optimizations, not only because it's hard to implement, but also because it often does not provide as much improvement as the first two. In practice, it is usually a wise decision not to implement the fanciest optimizations because they tend to be hard to implement, costly in compile time while not much payoff can be gained.
\section{Local optimization}
Local optimization focuses on a single basic block. There is no need to analyze the entire method body.
\subsection{Constant folding}
For an instruction $\mathtt{x\coloneqq y\:op\:z}$ in which \texttt{y,z} are both constants, $\mathtt{y\:op\:z}$ can be computed at compile time. E.g., $\mathtt{x\coloneqq 2 + 2\Rightarrow x\coloneqq 4}$.

Constants folding can be dangerous when the compiler and the target code it generates are run on different machines, which is not uncommon in reality, e.g. in most embedded platforms. The two machines might feature different round-offs of floating point numbers. If we do constant folding according to the floating point semantics of the compile machine directly at compile time, we may end up with unwanted result at runtime. An obvious solution is to keep full precision inside the compiler and represent floating pointer numbers as string literals, and leave it to the runtime machine to handle the round offs.
\subsection{Eliminate unreachable basic blocks}
By eliminating basic blocks that cannot be reached from the initial block, we can make the program smaller, and sometimes faster, because of cache effects.
\subsection{Common subexpression elimination}
Some optimizations can be simplified if each register occurs only once on the lhs of an assignment. Intermediate code can be rewritten into \textbf{single assignment form} by introducing new registers to substitute earlier appearances of registers that are assigned more than once. 

If a basic block is in single assignment form and a definition $\color{red}\mathtt{x\coloneqq}$ is the first use of \texttt{x} in a block, then when two assignments have the same rhs, they are guaranteed to compute the same value, which allows us saving the trouble of computing the same expression twice. We can simply substitute the second appearance of the rhs with the register assigned in its first appearance.
\subsection{Copy propagation}
if $\color{red}\mathtt{w\coloneqq x}$ appears in a block, subsequent use of \texttt{w} can be replaced with \texttt{x}.
\subsection{Dead instruction elimination}
if $\color{red}\mathtt{w\coloneqq rhs}$ appears in a basic block, and \texttt{w} does not appear anywhere else in the program, then the instruction is dead and can be eliminated.

Each local optimization does little by itself, but typically optimizations will interact with each other. Performing one optimization might enable another one. Thus the usual approach is to iterate until no more improvements can be made. The optimizer can also be stopped at any point to limit compilation time. 
\subsection{Peephole optimization}
Local optimizations can be applied directly to assembly code rather than to intermediate code. \textbf{Peephole optimization} is effective for improving assembly code. The ``peephole'' is a short sequence of contiguous instructions. The optimizer replaces the sequence with an equivalent but faster sequence. The process is repeated for maximum effect.
\section{Global optimization}
\subsection{Dataflow analysis}
In order to replace a use of \texttt{x} by a constant \texttt{k} we must make sure that on every path to the use of \texttt{x}, the last assignment to \texttt{x} is $\mathtt{x\coloneqq k}$. This condition is not trivial to check, considering the existence of loops and conditional branches. Global \textbf{dataflow analysis} is required to check this condition.

Global optimization tasks share several common traits:
\begin{itemize}
\item The optimization depends on knowing a property X at a particular point in program execution.
\item Proving X at any single point requires knowledge of the entire program.
\item It is OK to be conservative. If the optimization requires X being true, then we want to figure out whether X is definitely true or we don't know if X is true. It is always safe to say ``don't know''.
\end{itemize}
\subsection{Global constant propagation}
We use the following denotations to note the status of \texttt{x} at each program point:
\begin{description}
\item[$\bot$]: The statement never gets executed.
\item[C]: Constant C.
\item[$\top$]: \texttt{x} is not a constant.
\end{description}
Once we have the status of \texttt{x} at every program point, we can simply replace the use of \texttt{x} by the associated constant where the status of \texttt{x} is constant. In order to gain this information, we define a function $\mathtt{\color{red}C(x,s,in/out)}$ to compute information about the value of \texttt{x} before/after the statement \texttt{s}. In the following discussion, statement \texttt{s} has immediate predecessor statements $\mathtt{p_1,\dots,p_n}$. We have the following rules to infer the \texttt{in} status of one statement from the \texttt{out} of its predecessors.
\begin{enumerate}
\item If $\mathtt{C(p_i,x,out) = \top}$ for any \texttt{i}, then $\mathtt{C(s,x,in) = \top}$.
\item If $\mathtt{C(p_i,x,out) = c_i,\:C(p_j,x,out) = c_j,\:c_i\neq c_j}$, then $\mathtt{C(s,x,in) = \top}$.
\item If $\mathtt{C(p_i,x,out) = c}$ for at least one \texttt{i} and $\mathtt{C(p_i,x,out) = \bot}$ for all other \texttt{i}, then $\mathtt{C(s,x,in) = c}$.
\item If $\mathtt{C(p_i,x,out) = \bot}$ for all \texttt{i}, then $\mathtt{C(s,x,in) = \bot}$.  
\end{enumerate} 
Now we discuss the rules to infer the \texttt{out} status of a statement from its own \texttt{in} status.
\begin{enumerate}
\setcounter{enumi}{4}
\item If $\mathtt{C(s,x,in)=\bot}$, then $\mathtt{C(s,x,out)=\bot}$.
\item $\mathtt{C(x\coloneqq c,x,out)=c}$ if \texttt{c} is a constant.
\item $\mathtt{C(x\coloneqq f(\dots),x,out)=\top}$, if \texttt{f(\dots)} is not a constant.
\item If $\mathtt{y\neq x}$, then $\mathtt{C(y\coloneqq \dots,x,out)=C(y\coloneqq \dots,x,in)}$.
\end{enumerate}
We can gain information about status of \texttt{x} on all program points with the following algorithm:
\begin{enumerate}
\item For every entry \texttt{s} to the program, set $\mathtt{C(s,x,in)=\top}$.
\item Set $\mathtt{C(s,x,in/out)=\bot}$ everywhere else.
\item Repeat until the rules above are satisfied: pick statement \texttt{s} that does not satisfy the rules and update accordingly. 
\end{enumerate}

Special consideration needs to be taken when it comes to loops. If the \texttt{in} status of each statement relies on its predecessors, the reliance relation will form a cycle, making it impossible to obtain a result without assigning some initial values. The initial value $\bot$, which means ``by far the execution hasn't reached this point", is obviously a wise choice.

The rules above can actually be simplified by the idea of ordering among the 3 values : $\bot < \mathtt{C} < \top$. Note that different constants are not comparable to each other. Rules 1-4 can actually be written as 
\begin{equation*}
\mathtt{C(s,x,in)=\underset{i}{lub}(C(p_i,x,out))}
\end{equation*}
in which \texttt{lub} means least upper bound\footnote{We used this concept in the discussion of type checking}. The use of \texttt{lub} actually explains why the algorithm is guaranteed to terminate. All values start as $\bot$ and can only increase, thus the status at any point can change at most twice, which implies that the constant propagation algorithm is linear in program size: total number of steps $\leq$ 2 * Number of $\mathtt{C(\dots)}$ values to compute = 4 * Number of statements.
\subsection{Liveness analysis}
Once constants have been propagated, we would like to eliminate dead code. This involves the analysis of \textbf{liveness}. A variable \texttt{x} is live at statement \texttt{s} if 
\begin{itemize}
\item There exists a statement \texttt{s'} that uses \texttt{x}.
\item There is a path from \texttt{s} to \texttt{s'}.
\item That path has no intervening assignment to \texttt{x}.
\end{itemize}
A statement $\mathtt{x\coloneqq \dots}$ is dead code and can be removed if \texttt{x} is dead after the assignment. The propagation of liveness obeys the following rules:
\begin{enumerate}
\item $\mathtt{L(p,x,out) = \underset{i}{\cup}\:L(s_i,x,in)}$, in which $\mathtt{s_i}$ are successors of \texttt{p}.
\item $\mathtt{L(s,x,in)=true}$ if \texttt{s} refers to \texttt{x} on the rhs.
\item $\mathtt{L(x\coloneqq e,x,in)=false}$ if \texttt{e} does not refer to \texttt{x}.
\item $\mathtt{L(s,x,in)=L(s,x,out)}$ if \texttt{s} does not refer to \texttt{x}.
\end{enumerate}
We can obtain the liveness information of variables using the following algorithm, just as we did for constant propagation:
\begin{enumerate}
\item Initialize all $\mathtt{L(\dots)=false}$.
\item Repeat until all statements satisfy the rules above: pick \texttt{s} where the rules are not satisfied and update accordingly.
\end{enumerate}
A value can change from false to true but not the other way around, thus can change only once, which guarantees the termination of the algorithm.

Constant propagation is a forward analysis (information pushed from input to output), while liveness analysis is a backward analysis (information pushed from output to input). There exist other global dataflow analysis, and most of them can be classified as either forward or backward analysis. Most of them follow the methodology of \textbf{local rules relating information between adjacent program points}. 
\section{Register allocation}
Intermediate code uses unlimited temporaries, which simplifies code generation and optimization, but complicates final translation to assembly. The intermediate needs to be rewritten by assigning multiple temporaries to the same register without changing the behavior of the program, so that it uses no more temporaries than the number of machine registers. Register allocation is as old as compilers. 

The basic principle of register allocation is that \textbf{temporaries $t_1$ and $t_2$ can share the same register if at any point in the program at most one of them is live}. In other words, \textbf{if $t_1$ and $t_2$ are both live at any time of the program, they cannot share a register}.
\subsection{Register interference graph (RIG)}
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{liveness.jpg}
\caption{Liveness analysis of a program}\label{liveness}
\end{figure}

Take the program in Figure \ref{liveness} as an example. Its liveness analysis result is also shown. We can build an undirected graph according to this information. Each temporary is a node in the graph, and an edge is added between $t_1$ and $t_2$ if they are both live at some point in the program. The graph is called the \textbf{register interference graph (RIG)}. According to the principle above, two temporaries can share the same register as long as there is no edge between them in the RIG. The RIG of the program in Figure \ref{liveness} is shown in Figure \ref{rig}. In this example, \texttt{a} and \texttt{d} can use the same register since there is no edge between them.
\begin{figure}[ht]
\centering
\includegraphics[width=0.3\textwidth]{rig.jpg}
\caption{RIG}\label{rig}
\end{figure}

RIG extracts exactly the information needed to carry out register allocation, and it provides a global (i.e. over the entire control flow graph) picture of the requirements for register allocation. After the construction of the RIG, the register allocation algorithm that we will present is architecture independent.
\subsection{Graph coloring}
A \textbf{coloring of graph} is an assignment of colors to nodes such that nodes connected by an edge have different colors. A graph is \textbf{k-colorable} if it can be colored using k or fewer colors. In our problem, if the RIG is k-colorable, then there must exist a register assignment that uses no more than k machine registers. The RIG shown in Figure \ref{rig} is 4-colorable.

We will take a divide-and-conquer approach to solve the problem.
\begin{itemize}
\item Pick a node \texttt{t} with \textbf{fewer} than \texttt{k} neighbors in the RIG.
\item Eliminate \texttt{t} and its edges from the RIG.
\item If the rest of the RIG is \texttt{k}-colorable, then so is the original RIG.
\end{itemize}
The logic is quite intuitive. Let $c_1,\dots,c_n(n<k)$ be the colors assigned to the neighbors of \texttt{t} in the reduced graph, then we can pick a color from the $k-n$ colors unused by these neighbors to color \texttt{t} in the original RIG. The whole process can be described as below.
\begin{enumerate}
\item Build the stack of nodes.
\begin{itemize}
\item Pick a node \texttt{t} with fewer than \texttt{k} neighbors.
\item Push \texttt{t} on the stack and remove it from the RIG.
\item Repeat until the RIG is empty.
\end{itemize}
\item Assign colors to nodes.
\begin{itemize}
\item Pop the node on top of the stack.
\item Color the node with a color different from those assigned to its neighbors, which have been popped out of the stack and colored in previous steps.
\end{itemize}
\end{enumerate}
\subsection{Spilling}
When the heuristic fails to find a coloring, it is impossible to hold all temporaries in the registers. Some of them have to be \textbf{spilled} to the memory. 

When we get stuck in the heuristic, we will find that all nodes left in the RIG have \texttt{k} or more neighbors. We need to choose a node as a candidate for spilling. The associated value might have to live in the memory. The chosen node \texttt{f} and its edges will be removed from the RIG, and we continue the coloring with the rest of the RIG. If the coloring ends up with success, we eventually have to assign a color to \texttt{f}. If its (\texttt{k} or more) neighbors use fewer than \texttt{k} colors, a color can easily be assigned, which is the case of \textbf{optimistic coloring}. If optimistic coloring fails, \texttt{f} has to be spilled. We need to:
\begin{itemize}
\item Allocate a memory location, typically in the current frame, for \texttt{f}, noted as \texttt{fa}.
\item Before each operation that reads \texttt{f}, insert \texttt{f $\coloneqq$ load fa}.
\item After each operation that writes \texttt{f}, insert \texttt{store f, fa}.
\end{itemize} 
Note that different \texttt{load}s and \texttt{store}s of \texttt{f} do not have to use the same register, thus occurrences of \texttt{f} should be indexed (\texttt{f1, f2, }etc.). The liveness information needs to be recalculated. The new liveness information is almost the same as the old one, except that \texttt{f} has been split into several different temporaries. \texttt{fi} is live only between a \texttt{fi $\coloneqq$ load fa} and the next instruction (which reads it), and between a \texttt{store fi, fa} and the preceding instruction (which writes it). Obviously, spilling reduces the interferences of \texttt{f} with other temporaries, resulting in a RIG in which each \texttt{fi} has fewer neighbors than the original \texttt{f}.

It is not uncommon that additional spilling is required before a coloring is found. The tricky part of spilling is the choice of the node to spill. There is no best choice, but it might well be helpful to spill the temporary with the most conflicts, which is the most promising choice for obtaining a colorable RIG, or the one with few definitions and uses so that the number of resulting memory operations can be minimized. Note that spilling temporaries in inner loops should be avoided for efficiency. 
\section{Cache management}
Recall the memory hierarchy of modern computers: registers(KB) $\rightarrow$ cache(MB) $\rightarrow$ main memory(GB) $\rightarrow$ disk(TB). The access to caches is much faster than that to the main memory, but the cost of a cache miss is very high. Thus caches need to be properly managed. Compilers are good at managing registers, but there is little that they are able to do to manage caches, which is a job mostly left for the programmers.

Consider the following piece of code.
\begin{lstlisting}
for(int i = 0; i < 10; ++i)
	for(int j = 0; j < 1000000; ++j)
		a[i] *= b[i];
\end{lstlisting}
Clearly every access to \texttt{a[i]} and \texttt{b[i]} is a cache miss. If we switch the two loops:
\begin{lstlisting}
for(int j = 0; j < 1000000; ++j)
	for(int i = 0; i < 10; ++i)
		a[i] *= b[i];
\end{lstlisting}
The cache behavior becomes much better. Some, but not all, compilers implement this optimization to achieve better management of caches. Note that it's not always easy for the compiler to figure out whether the switch of loops is legal or not.
\section{Automatic memory management (GC)}

Storage management is still a hard problem in modern programming. C and C++ use manual storage management, which results in many storage bugs: memory leak, deference of dangling pointers, overwriting parts of a data structure by accident, etc. Such bugs are hard to find because they often do not show visible effect until far away in time and space. Heavy efforts have been made to develop a series of techniques for completely automatic memory management since the 1950s. But it didn't become mainstream until the popularity of JAVA in the 1990s. 

The basic principle of \textbf{automatic memory management}, or \textbf{garbage collection} is simple. Memory is allocated when an object is created, and this piece of memory will be reclaimed for future allocation once the object can no longer be used again by the program. 

Intuitively, a program can only use the objects that it can find. A formal definition is the \textbf{reachability} of an object. An object \texttt{x} is reachable if and only if a register contains a pointer to \texttt{x}, or another reachable object contains a pointer to \texttt{x}. All reachable objects can be found by starting from registers and following all the pointers, which requires knowledge of the AR. An unreachable object can never be used again, and is called \textbf{garbage}. Note that reachability guarantees the possibility of future uses rather than actual future uses. Thus reachable objects are actually an approximation of objects that will be used later.  

We will present a few mainstream GC methods in the rest of this section.
\subsection{Mark and sweep}
The \textbf{mark and sweep} method executes 2 phases when memory runs out:
\begin{description}
\item[the mark phase]traces all reachable objects;
\item[the sweep phase]collects garbage objects.
\end{description}
Every object will have an extra bit reserved for memory management called \textbf{the mark bit}, which is initialized to 0 and gets set to 1 during the mark phase if the object is reachable. The mark phase is shown in Algorithm \ref{markphase}.
\begin{algorithm}
\caption{The mark phase}\label{markphase}
\begin{algorithmic}
\State{\textbf{let} todo = \{all roots\}}
\While{todo $\neq$ \o}
\State{pick v $\in$ todo}
\State{todo $\leftarrow$ todo - \{v\}}
\If {mark(v) = 0}
\State{mark(v) $\leftarrow$ 1}
\State{\textbf{let} $v_1,\dots,v_n$ = the pointers contained in v}
\State{todo $\leftarrow$ todo$\:\cup\:\{v_1,\dots,v_n\}$}
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

The sweep phase scans the heap looking for objects with mark bit 0, who are then added to the free list as garbage to be collected later. Objects with mark bit 1 have their mark bit reset to 0. The process is shown in Algorithm \ref{sweepphase}.
\begin{algorithm}
\caption{The sweep phase}\label{sweepphase}
\begin{algorithmic}
\State{p $\leftarrow$ bottom of heap}
\While{p $<$ top of heap}
\If{mark(p) = 1}
\State{mark(p) = 0}
\Else\State{add block [p$\dots$p + sizeof(p) - 1] to free list}
\EndIf
p $\leftarrow$ p + sizeof(p)
\EndWhile
\end{algorithmic}
\end{algorithm}

The conception of the mark and sweep algorithm is simple, but lie behind the algorithm are a number of tricky details typical of GC algorithms. 

The mark phase is invoked when we are out of memory, yet space is needed to construct the \texttt{todo} list, whose size is unbounded and thus for which space cannot be reserved in advance. A trick to solve the problem is \textbf{pointer reversal}: when a pointer is followed during reachability analysis, it is reversed to point to its parent. Essentially it helps to maintain the stack for a depth-first search of the graph. Similarly, the free list is stored in the free objects themselves.

The space for a new object is allocated from the free list. A block large enough is picked, an area of necessary size is allocated from it, and the leftover is put back in the free list. The algorithm can fragment the memory. It is necessary to merge the block when possible. Objects are not moved, thus there is no need to update pointers to objects, and it works for languages like C/C++, in which the literal value of a pointer is part of the language semantics.
\subsection{Stop and copy}
In \textbf{stop and copy}, memory is organized into two parts: the old space used for allocation and the new space used as a reserver for GC. The heap pointer points to the next available word in the old space, and an allocation does nothing but advancing the heap pointer.

When the old space is full, the program is suspended, and all reachable objects are copied from the old space into the new space. Garbage (unreachable objects) is left behind and no longer occupies any space in the new space. After the copy, the roles of the old and new spaces are switched and the program is resumed. 

Since objects are physically moved, we have to fix all pointers to an object after copying it. As a solution, we store in the old object a \textbf{forwarding pointer} to the new object after it gets copied. When a pointer takes us to an object with a forwarding pointer, we can be aware of the fact that it has been moved, and fix the pointer accordingly. 

The traversal can be implemented without using extra space. We partition the new space into three contiguous regions with two pointers: the \texttt{scan} pointer and the \texttt{alloc} pointer. The \texttt{alloc} pointer points to the first empty word that has not been occupied by the copied objects. The \texttt{scan} pointer is between copied objects whose pointers have been followed and those whose pointers haven't. The algorithm is shown in Algorithm \ref{stopandcopy}.
\begin{algorithm}
\caption{Stop and copy}\label{stopandcopy}
\begin{algorithmic}
\While{scan $\neq$ alloc}
\State{\textbf{let} O be the object at scan pointer}
\ForAll{pointer p contained in O}
\State{find O' that p points to}
\If{O' is without a forwarding pointer}
\State{copy O' to the new space (update alloc pointer)}
\State{set a word of old O' to point to the new copy}
\State{(set up the forwarding pointer)}
\State{change p to point to the new copy of O'}
\Else
\State{set p in O equal to the forwarding pointer}
\EndIf
\EndFor
\State{increment scan pointer to the next object}
\EndWhile
\end{algorithmic} 
\end{algorithm} 
As with mark and sweep, we have to have knowledge of how large an object is and where the pointers are stored inside it when we scan it. The latter can be unavailable for some languages, e.g. C++. In such case, we take a conservative approach: if a memory word looks like a pointer, which requires it being aligned and pointing to a valid address in the data segment, it is considered as a pointer. The set of reachable objects is overestimated. But still the objects cannot be copied and moved. And note that objects pointed to by the stack also need to be scanned and copied, which can turn out an expensive operation because the entire stack needs to be scanned.

Stop and copy is generally considered to be the fastest GC algorithm, because allocation is very cheap, and collection is relatively cheap, especially when there are a lot of garbage. But some language like C/C++ does not allow object copying, and this method cannot apply to them.
\subsection{Reference counting}
Rather than wait for memory to be exhausted, \textbf{reference counting} tries to collect an object immediately when there are no pointers to it. The number of pointers to an object (the reference count) is saved in it, and every assignment operation manipulates the reference count. 

\texttt{new} returns an object with reference count equal to 1. Let \texttt{rc(o)} be the reference count of object \texttt{o}. Each assignment $\mathtt{x\leftarrow y}$ becomes 
\begin{align*}
&\mathtt{rc(o_y)\leftarrow rc(o_y) + 1}\\
&\mathtt{rc(o_x)\leftarrow rc(o_x) - 1}\\
&\mathtt{if(rc(o_x) == 0)\:free\:x}\\
&\mathtt{x\leftarrow y}\\
\end{align*}

Reference counting is easy to implement. There will not be long pauses in the execution for the sake of GC. But it cannot handle circular structures, and manipulating reference count at each assignment is quite slow. 

Automatic memory management prevents storage bugs, but reduces programmer's control. The possible problematic pauses caused by GC are sometimes intolerable for real-time application. And memory leaks are still possible, or even likely even if GC techniques are used. A common type of error made by programmers, not a grammatical error but an error in terms of engineering, is to forget to set a pointer to \texttt{Null} when the pointer will clearly never be used again: the object won't be recognized as garbage until the pointer gets out of range, which is too late.
\ifx\PREAMBLE\undefined
\end{document}
\fi